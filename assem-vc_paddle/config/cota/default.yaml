train:
  num_workers: 16
  batch_size: 64
  adam:
    lr: 0.0003
    weight_decay: 0.000001
  mask_padding: True
  decay:
    rate: 0.05
    start: 25000
    end: 50000
  grad_clip: 1.0  # 0 for no gradient clipping
  teacher_force:
    rate: 0.5 # 0.5 is the most stable value
  use_attn_loss: True # Using guided attention loss (See README.md)
###########################
log:
  chkpt_dir: 'chkpt/cota'
  log_dir: 'logs/cota'
